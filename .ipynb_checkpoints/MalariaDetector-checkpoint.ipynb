{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Giott\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPool2D , Conv2D , Flatten , Dense , MaxPooling2D ,Dropout\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Giott\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.add(Conv2D(filters=15 , kernel_size= 2 , padding='valid' , activation='relu' , input_shape=(200,200,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 199, 199, 15)      75        \n",
      "=================================================================\n",
      "Total params: 75\n",
      "Trainable params: 75\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Giott\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 141,873\n",
      "Trainable params: 141,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128 , activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense (units=1  , activation='sigmoid'))\n",
    "model.compile(optimizer='adam' , loss='binary_crossentropy' , metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 16)        208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 32)        2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 64)          8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               131200    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 141,873\n",
      "Trainable params: 141,873\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as k \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import  ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parasite_datagen = image.ImageDataGenerator(rescale= 1./255 , shear_range=0.2 , zoom_range=0.2 , horizontal_flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27558 images belonging to 2 classes.\n",
      "Found 27558 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "parasite_datagen = ImageDataGenerator(rescale=1./255,shear_range=0.2,zoom_range=0.2,horizontal_flip=True)\n",
    "uninfected_datagen = ImageDataGenerator(rescale=1./255)\n",
    "parasite_data = parasite_datagen.flow_from_directory('images/cell_images',\n",
    "                                                     target_size=(32,32),\n",
    "                                                     batch_size=32,\n",
    "                                                     class_mode = 'binary')\n",
    "uninfected_data = uninfected_datagen.flow_from_directory('images/cell_images',\n",
    "                                                        target_size=(32,32),\n",
    "                                                        batch_size=32,\n",
    "                                                        class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Giott\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 0.6472 - acc: 0.6272 - val_loss: 0.5949 - val_acc: 0.6856\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.59489, saving model to model.weights.best.hdf5\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.5727 - acc: 0.7159 - val_loss: 0.5114 - val_acc: 0.7775\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.59489 to 0.51144, saving model to model.weights.best.hdf5\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 16s 163ms/step - loss: 0.4689 - acc: 0.7991 - val_loss: 0.4988 - val_acc: 0.7575\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.51144 to 0.49881, saving model to model.weights.best.hdf5\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.3882 - acc: 0.8438 - val_loss: 0.2757 - val_acc: 0.8794\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.49881 to 0.27571, saving model to model.weights.best.hdf5\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.2730 - acc: 0.8994 - val_loss: 0.1839 - val_acc: 0.9275\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.27571 to 0.18389, saving model to model.weights.best.hdf5\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 16s 165ms/step - loss: 0.2221 - acc: 0.9213 - val_loss: 0.1625 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.18389 to 0.16255, saving model to model.weights.best.hdf5\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.2177 - acc: 0.9281 - val_loss: 0.1545 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.16255 to 0.15446, saving model to model.weights.best.hdf5\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 16s 165ms/step - loss: 0.1933 - acc: 0.9322 - val_loss: 0.1591 - val_acc: 0.9500\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.15446\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.1784 - acc: 0.9412 - val_loss: 0.1627 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.15446\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 16s 164ms/step - loss: 0.2051 - acc: 0.9347 - val_loss: 0.1558 - val_acc: 0.9519\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.15446\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 17s 165ms/step - loss: 0.1845 - acc: 0.9450 - val_loss: 0.1547 - val_acc: 0.9506\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.15446\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1749 - acc: 0.9466 - val_loss: 0.1593 - val_acc: 0.9481\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.15446\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 17s 166ms/step - loss: 0.1735 - acc: 0.9475 - val_loss: 0.1737 - val_acc: 0.9456\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.15446\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1694 - acc: 0.9472 - val_loss: 0.1773 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.15446\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1745 - acc: 0.9497 - val_loss: 0.1506 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.15446 to 0.15055, saving model to model.weights.best.hdf5\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1953 - acc: 0.9372 - val_loss: 0.1200 - val_acc: 0.9644\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.15055 to 0.12003, saving model to model.weights.best.hdf5\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1750 - acc: 0.9428 - val_loss: 0.1547 - val_acc: 0.9444\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.12003\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1644 - acc: 0.9469 - val_loss: 0.1398 - val_acc: 0.9587\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.12003\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1772 - acc: 0.9441 - val_loss: 0.1525 - val_acc: 0.9487\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.12003\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1466 - acc: 0.9537 - val_loss: 0.1520 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.12003\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1545 - acc: 0.9497 - val_loss: 0.1385 - val_acc: 0.9537\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.12003\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1773 - acc: 0.9497 - val_loss: 0.1640 - val_acc: 0.9494\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.12003\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1562 - acc: 0.9525 - val_loss: 0.1549 - val_acc: 0.9513\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.12003\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.1455 - acc: 0.9569 - val_loss: 0.1603 - val_acc: 0.9531\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.12003\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1633 - acc: 0.9453 - val_loss: 0.1379 - val_acc: 0.9525\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.12003\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.1484 - acc: 0.9519 - val_loss: 0.1880 - val_acc: 0.9381\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.12003\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1678 - acc: 0.9481 - val_loss: 0.1193 - val_acc: 0.9544\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.12003 to 0.11931, saving model to model.weights.best.hdf5\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.1631 - acc: 0.9472 - val_loss: 0.1117 - val_acc: 0.9681\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.11931 to 0.11173, saving model to model.weights.best.hdf5\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.1464 - acc: 0.9597 - val_loss: 0.1111 - val_acc: 0.9631\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.11173 to 0.11113, saving model to model.weights.best.hdf5\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 16s 160ms/step - loss: 0.1492 - acc: 0.9509 - val_loss: 0.1419 - val_acc: 0.9575\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.11113\n"
     ]
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='model.weights.best.hdf5', verbose=1, save_best_only=True)\n",
    "#using GPU\n",
    "with tf.device('/gpu:0'):\n",
    "    trained_model = model.fit_generator(parasite_data , steps_per_epoch=100,epochs=30 ,callbacks=[checkpointer] , validation_data=uninfected_data  , validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.594888</td>\n",
       "      <td>0.685625</td>\n",
       "      <td>0.647182</td>\n",
       "      <td>0.627188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.511436</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>0.572717</td>\n",
       "      <td>0.715938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.498813</td>\n",
       "      <td>0.757500</td>\n",
       "      <td>0.468892</td>\n",
       "      <td>0.799063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.275714</td>\n",
       "      <td>0.879375</td>\n",
       "      <td>0.388176</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.183887</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.273034</td>\n",
       "      <td>0.899375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.162549</td>\n",
       "      <td>0.945625</td>\n",
       "      <td>0.222117</td>\n",
       "      <td>0.921250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.154458</td>\n",
       "      <td>0.951875</td>\n",
       "      <td>0.217715</td>\n",
       "      <td>0.928125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.159055</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.193333</td>\n",
       "      <td>0.932187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.162708</td>\n",
       "      <td>0.950625</td>\n",
       "      <td>0.179529</td>\n",
       "      <td>0.940769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.155762</td>\n",
       "      <td>0.951875</td>\n",
       "      <td>0.205054</td>\n",
       "      <td>0.934688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.154710</td>\n",
       "      <td>0.950625</td>\n",
       "      <td>0.184455</td>\n",
       "      <td>0.945000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.159276</td>\n",
       "      <td>0.948125</td>\n",
       "      <td>0.174895</td>\n",
       "      <td>0.946562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.173720</td>\n",
       "      <td>0.945625</td>\n",
       "      <td>0.173454</td>\n",
       "      <td>0.947500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.177288</td>\n",
       "      <td>0.944375</td>\n",
       "      <td>0.169439</td>\n",
       "      <td>0.947187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.150552</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.174473</td>\n",
       "      <td>0.949688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.120028</td>\n",
       "      <td>0.964375</td>\n",
       "      <td>0.195346</td>\n",
       "      <td>0.937187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.154740</td>\n",
       "      <td>0.944375</td>\n",
       "      <td>0.174975</td>\n",
       "      <td>0.942813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.139825</td>\n",
       "      <td>0.958704</td>\n",
       "      <td>0.165493</td>\n",
       "      <td>0.946440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.152465</td>\n",
       "      <td>0.948750</td>\n",
       "      <td>0.177195</td>\n",
       "      <td>0.944063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.152045</td>\n",
       "      <td>0.951250</td>\n",
       "      <td>0.146555</td>\n",
       "      <td>0.953750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.138523</td>\n",
       "      <td>0.953750</td>\n",
       "      <td>0.154491</td>\n",
       "      <td>0.949688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.164007</td>\n",
       "      <td>0.949375</td>\n",
       "      <td>0.177305</td>\n",
       "      <td>0.949688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.154878</td>\n",
       "      <td>0.951250</td>\n",
       "      <td>0.156245</td>\n",
       "      <td>0.952500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.160308</td>\n",
       "      <td>0.953125</td>\n",
       "      <td>0.145539</td>\n",
       "      <td>0.956875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.137910</td>\n",
       "      <td>0.952500</td>\n",
       "      <td>0.163333</td>\n",
       "      <td>0.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.187960</td>\n",
       "      <td>0.938125</td>\n",
       "      <td>0.149366</td>\n",
       "      <td>0.951481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.119308</td>\n",
       "      <td>0.954375</td>\n",
       "      <td>0.167826</td>\n",
       "      <td>0.948125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.111727</td>\n",
       "      <td>0.968125</td>\n",
       "      <td>0.163113</td>\n",
       "      <td>0.947187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.111125</td>\n",
       "      <td>0.963125</td>\n",
       "      <td>0.146421</td>\n",
       "      <td>0.959688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.141931</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.149243</td>\n",
       "      <td>0.950937</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs  val_loss   val_acc      loss       acc\n",
       "0        0  0.594888  0.685625  0.647182  0.627188\n",
       "1        1  0.511436  0.777500  0.572717  0.715938\n",
       "2        2  0.498813  0.757500  0.468892  0.799063\n",
       "3        3  0.275714  0.879375  0.388176  0.843750\n",
       "4        4  0.183887  0.927500  0.273034  0.899375\n",
       "5        5  0.162549  0.945625  0.222117  0.921250\n",
       "6        6  0.154458  0.951875  0.217715  0.928125\n",
       "7        7  0.159055  0.950000  0.193333  0.932187\n",
       "8        8  0.162708  0.950625  0.179529  0.940769\n",
       "9        9  0.155762  0.951875  0.205054  0.934688\n",
       "10      10  0.154710  0.950625  0.184455  0.945000\n",
       "11      11  0.159276  0.948125  0.174895  0.946562\n",
       "12      12  0.173720  0.945625  0.173454  0.947500\n",
       "13      13  0.177288  0.944375  0.169439  0.947187\n",
       "14      14  0.150552  0.953125  0.174473  0.949688\n",
       "15      15  0.120028  0.964375  0.195346  0.937187\n",
       "16      16  0.154740  0.944375  0.174975  0.942813\n",
       "17      17  0.139825  0.958704  0.165493  0.946440\n",
       "18      18  0.152465  0.948750  0.177195  0.944063\n",
       "19      19  0.152045  0.951250  0.146555  0.953750\n",
       "20      20  0.138523  0.953750  0.154491  0.949688\n",
       "21      21  0.164007  0.949375  0.177305  0.949688\n",
       "22      22  0.154878  0.951250  0.156245  0.952500\n",
       "23      23  0.160308  0.953125  0.145539  0.956875\n",
       "24      24  0.137910  0.952500  0.163333  0.945312\n",
       "25      25  0.187960  0.938125  0.149366  0.951481\n",
       "26      26  0.119308  0.954375  0.167826  0.948125\n",
       "27      27  0.111727  0.968125  0.163113  0.947187\n",
       "28      28  0.111125  0.963125  0.146421  0.959688\n",
       "29      29  0.141931  0.957500  0.149243  0.950937"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = pd.DataFrame.from_dict(trained_model.history)\n",
    "acc = pd.concat([pd.Series(range(0,30),name='epochs'),acc],axis=1)\n",
    "acc.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
